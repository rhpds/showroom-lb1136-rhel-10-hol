
= *Virtualization Management* (libvirt)

[discrete]
== *Skill Level: Intermediate* icon:check[]




WARNING: This exercise depends on the work created by the Image Builder unit.  
If you have not completed that work, please do so before proceeding.

Provided your hardware is reasonably modern, chances are that it supports virtualization.  This unit introduces simple virtualization management using kvm and libvirt.  You will learn how to:

    * Install additional necessary software
    * Enable necessary system services and firewall ports
    * Use the command line to create and manage a virtual machine
    * Use the web console (cockpit) to create and manage a virtual machine

== Getting Started

For these exercises, you will be using the host `node3` as user `root`.

From host `bastion`, ssh to `node3`.

[{format_cmd}]
----
ssh node3
----

Use `sudo` to elevate your privileges.

[{format_cmd}]
----
[[ "$UID" == 0 ]] || sudo -i
----

Verify that you are on the right host for these exercises.

[{format_cmd}]
----
workshop-virt-checkhost.sh
----

You are now ready to proceed with these exercises.

== Requirements

First we need to ensure the system being used supports either:

    * Intel VT-x and Intel 64 virtualization extensions
    * AMD-V and the AMD64 virtualization extensions

This is done with the following simple commands.

You can start by examining the CPU flags (capabilities) advertised by your system.

[{format_cmd}]
----
grep -E 'svm|vmx' /proc/cpuinfo
----

[{format_output}]
----
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 
clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good
nopl xtopology cpuid tsc_known_freq pni pclmulqdq *vmx* ssse3 fma cx16 pcid sse4_1 sse4_2 
x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnow
prefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept
vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rds
eed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat pku os
pke avx512_vnni md_clear arch_capabilities
----

You are looking for either the Intel flag (vmx) or the AMD flag (svm).  A more sophisticated command makes it a little easier to determine.

[{format_cmd}]
----
if grep -qE 'svm|vmx' /proc/cpuinfo ; then echo 'Virt Supported' ; else echo 'WARNING: no hardware virtualization support detected'; fi
----

[{format_output}]
----
Virt Supported
----

After you install all the required software, there are some additional tools to provide more detailed reporting on system capabilities.

WARNING:  It could very well be that your environment does not support 
native or nested virtualization.  This does not mean that this exercise 
will not work, but it does mean that (if it does work) it will be VERY slow.


== Installation and Configuration

NOTE: Please note that all software has been pre-installed and configured.  These steps are provided as reference material only.

The system needs to be configured with access to the following repos:

  * rhel-10-baseos-rpms
  * rhel-10-appstream-rpms

Install the required packages.

[{format_cmd}]
----
dnf install -y qemu-kvm libvirt virt-install libvirt-client libguestfs-tools cockpit-machines
----

Next we need to enable the libvirtd service.

[{format_cmd}]
----
systemctl enable --now libvirtd
----

NOTE: The "enable --now" syntax was introduced in RHEL 8 and allows for permanently enabling as well as immediately starting services in a single command.

Finally check the service status.

[{format_cmd}]
----
systemctl --no-pager status libvirtd
----

=== Verify Virtualization Host Status

One simple command checks various hardware and software configurations for support of virtualization.

[{format_cmd}]
----
virt-host-validate
----

[{format_output}]
----
  QEMU: Checking for hardware virtualization                                 : PASS
  QEMU: Checking if device /dev/kvm exists                                   : PASS
  QEMU: Checking if device /dev/kvm is accessible                            : PASS
  QEMU: Checking if device /dev/vhost-net exists                             : PASS
  QEMU: Checking if device /dev/net/tun exists                               : PASS
  QEMU: Checking for cgroup 'cpu' controller support                         : PASS
  QEMU: Checking for cgroup 'cpuacct' controller support                     : PASS
  QEMU: Checking for cgroup 'cpuset' controller support                      : PASS
  QEMU: Checking for cgroup 'memory' controller support                      : PASS
  QEMU: Checking for cgroup 'devices' controller support                     : PASS
  QEMU: Checking for cgroup 'blkio' controller support                       : PASS
  QEMU: Checking for device assignment IOMMU support                         : WARN (No ACPI IVRS table found, IOMMU either disabled in BIOS or not supported by this hardware platform)
----



== Deploy A Virtual-Machine with 'virt-install'

Red Hat provides pre-made generic images of RHEL for use as virtual machines in a QCOW2 format.

However, in order to access them for download one needs to have an active Red Hat Enterprise Linux entitlement.  An alternative to downloading a qcow image is to make one.  

Fortunately, that's precisely what you did in the previous unit with Image Builder.

=== Locate your QCOW Image

In the previous exercise, you built a custom QCOW2 image using Image Builder.  The result of that work should be a VM image named vmguest.qcow2 

[{format_cmd}]
----
ls /var/lib/libvirt/images
----

[{format_output}]
----
vmguest.qcow2
----

== Customize your QCOW Image

Now you need to do a few more things to your image:

    * set a hostname
    * set a root password
    * copy a simple HTML file
    * SELinux file relabeling in the guest
    * remove the cloud-init package


We need to capture the filename of our public SSH key.

[{format_cmd}]
----
export PUBKEY=$(ls ~/.ssh/*.pub | head -1)
----

Now you can proceed to customize the virt image.

We will include a timer on this command in order to measure how long this process takes.  

NOTE: For the resources used at the 2025 Red Hat Summit, this process runs for about 30 seconds.

[{format_cmd}]
----
time virt-customize \
    -a /var/lib/libvirt/images/vmguest.qcow2 \
    --hostname vmguest \
    --root-password password:redhat \
    --ssh-inject root:file:${PUBKEY} \
    --copy-in /usr/local/etc/index.html:/var/www/html \
    --selinux-relabel \
    --run-command 'echo "PermitRootLogin yes" >> /etc/ssh/sshd_config.d/rootlogin.conf' \
    --uninstall cloud-init
----

[{format_output}]
----
[   0.0] Examining the guest ...
[  19.1] Setting a random seed
[  19.2] Setting the machine ID in /etc/machine-id
[  19.2] Setting the hostname: vmguest
[  19.2] SSH key inject: root
[  20.7] Copying: /usr/local/etc/index.html to /var/www/html
[  20.7] Running: echo "PermitRootLogin yes" >> /etc/ssh/sshd_config.d/rootlogin.conf
[  20.8] Uninstalling packages: cloud-init
[  24.0] Setting passwords
[  25.2] SELinux relabelling
[  30.5] Finishing off

real    0m30.882s
user    0m0.884s
sys     0m1.583s
----


== VM Deployment

NOTE:  Please note that if your workshop environment did NOT show support for native or nested
virtualization, every step beyond this point will likely take a very long time (if it works at all).

It is now time to launch the VM

[{format_cmd}]
----
virt-install \
   --import \
   --name vmguest \
   --memory 2048 \
   --cpu host \
   --vcpus 1 \
   --disk /var/lib/libvirt/images/vmguest.qcow2 \
   --graphics vnc \
   --noautoconsole\
   --os-variant rhel9.0
----

Give the VM a few moments to boot.

NOTE: If you explored the web-console exercise, you can use cockpit to 
access the VM's console and see what's going on.  Just be sure you 
selected administrative access.

== Virtual Machine Connectivity

To determine what IP address was assigned to the new host, we can use the virsh utility.

[{format_cmd}]
----
virsh net-dhcp-leases default
----

The output will show us the clients MAC address and the IP address it was assigned via the libvirt integrated dnsmasq service.

[{format_output}]
----
 Expiry Time           MAC address         Protocol   IP address          Hostname   Client ID or DUID
-----------------------------------------------------------------------------------------------------------
 2021-11-13 11:19:33   52:54:00:63:85:76   ipv4       192.168.122.62/24   -          01:52:54:00:63:85:76
----

Another mechanism determine the IP address of the client is to use the 'domifaddr' option.

[{format_cmd}]
----
virsh domifaddr vmguest
----

[{format_output}]
----
 Name       MAC address          Protocol     Address
-------------------------------------------------------------------------------
 vnet0      52:54:00:63:85:76    ipv4         192.168.122.62/24
----


WARNING: Before you proceed, empty data in the above commands is an indication that the virtual machine has 
not completed its bootstrap.  Just give it a few more moments and try again.

Now that we have the network information, it is time to connect to the host. 

[{format_cmd}]
----
export VM_IP=$(virsh domifaddr vmguest | sed -e '1,2d' -e '$d' | awk '{ split($4,a,/\//) ; print a[1] }')
----

[{format_cmd}]
----
curl $VM_IP
----

[{format_plain}]
----
*** Success !!! It Works  ***
----



== Virtual Machine Inspection

Now it is time to connect to the host and check out some its characteristics.

[{format_cmd}]
----
export VM_IP=$(virsh domifaddr vmguest | sed -e '1,2d' -e '$d' | awk '{ split($4,a,/\//) ; print a[1] }')
----

[{format_cmd}]
----
ssh root@$VM_IP -o "StrictHostKeyChecking no"
----

The root password was set to 'redhat' in a previous step using the virt-customize command. However, since we injected an SSH public key at the same time, you should not be prompted for a password.

The virtual machine is on a private network and not accessbile from the internet.  You will only 
be able to access it from the bastion via SSH, or from the web console.

Verify that the httpd daemon is running.

[{format_cmd}]
----
systemctl --no-pager status httpd
----

Verify that `index.html` exists.

[{format_cmd}]
----
ls /var/www/html/index.html
----

Exit back to the host.

[{format_cmd}]
----
exit
----




== Additional CLI Commands

Some additional simple virtual machine management commands:

  * *virsh list* lists running virtual machines
  * *virsh list --all* lists all virtual machines regardless of state
  * *virsh start <vm-name>* starts a virtual machine
  * *virsh shutdown <vm-name>* performs a soft shutdown of the virtual machine
  * *virsh destroy <vm-name>* performs destructive cold stop of the virtual machine

== Explore VM Management with Web Console

Use the following URL:

  * link:{workshop_cockpit_url}[,window=_blank]

And the following UserID and Password:

[{format_cmd}]
----
{workshop_cockpit_user_name}
----

[{format_cmd}]
----
{workshop_cockpit_user_password}
----

Select *Virtual machines* from the menu on the left.  You will notice that the interface is pretty rudimentary compared to a modern virtualization manager like OpenShift Virtualization, but one critical feature is available: the console.

Take some time to explore the capabilities of the Web Console Virtual machines interface.




== Cleanup

WARNING: It is IMPORTANT to shutdown the deployed VMs.  Leaving any VM running in this workshop environment can adversely impact other exercises.

Using either the CLI (or the Web-Console), be sure to shutdown and remove the VM(s) you deployed to ensure additional workshop exercises perform reasonably.

[{format_cmd}]
----
virsh destroy vmguest

virsh undefine vmguest --remove-all-storage
----




== Conclusion

This concludes the exercises related to virtualization.

Time to finish this unit and return the shell to its home position.

[{format_cmd}]
----
workshop-finish-exercise.sh
----


== Additional Resources

Cockpit Project Page

    * link:http://cockpit-project.org/blog/category/release.html[Cockpit Project]

[discrete]
== End of Unit

////
Always end files with a blank line to avoid include problems.
////

